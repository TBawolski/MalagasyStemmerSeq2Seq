{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50c82cb9-72f1-4f1d-896e-a7455a1ac7f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rule-Based Stemming Demo:\n",
      "\n",
      "Word: mankabidy       => Stem: bidy\n",
      "Word: hankabidy       => Stem: bidy\n",
      "Word: mambabo         => Stem: babo\n",
      "Word: abiliana        => Stem: bi\n",
      "Word: mialana         => Stem: \n",
      "Word: zaka-tsaka      => Stem: -tsaka\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "\n",
    "#####################################################\n",
    "# 1. LOAD AFFIXES FROM CSV\n",
    "#####################################################\n",
    "\n",
    "def load_affixes_from_csv(filepath):\n",
    "    affixes = []\n",
    "    with open(filepath, mode='r', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            affix_str = row['affix'].strip()\n",
    "            affix_type = int(row['type'])\n",
    "            affixes.append({\n",
    "                'affix': affix_str,\n",
    "                'type': affix_type\n",
    "            })\n",
    "    return affixes\n",
    "\n",
    "\n",
    "#####################################################\n",
    "# 2. CONSONANT MUTATION REVERSAL LOGIC (SKELETON)\n",
    "#####################################################\n",
    "\n",
    "# This dict tries to map a *mutated* boundary back to possible original\n",
    "# prefix+root letter combos. In real practice, you'll need more nuance.\n",
    "revert_boundary_map = {\n",
    "    \"mb\":  [(\"n\", \"b\")],\n",
    "    \"mp\":  [(\"n\", \"p\")],\n",
    "    \"ng\":  [(\"n\", \"h\")],\n",
    "    \"nk\":  [(\"n\", \"h\")],\n",
    "    \"nd\":  [(\"n\", \"l\")],\n",
    "    \"m\":   [(\"n\", \"p\"), (\"n\", \"b\"), (\"n\", \"f\"), (\"n\", \"v\"), (\"n\", \"m\")],\n",
    "    \"n\":   [(\"n\", \"n\")],\n",
    "    \"ndr\": [(\"n\", \"r\")],\n",
    "    \"nts\": [(\"n\", \"s\")],\n",
    "    \"nj\":  [(\"n\", \"j\")],\n",
    "    # You can add more, especially for hyphen-based changes in compounds\n",
    "}\n",
    "\n",
    "\n",
    "#####################################################\n",
    "# 3. TENSE VARIANTS FOR PREFIXES (SKELETON)\n",
    "#####################################################\n",
    "\n",
    "def generate_tense_variants(prefix_base):\n",
    "    \"\"\"\n",
    "    Given a base prefix (e.g. \"manka\"), generate possible\n",
    "    tense/polarity variants (e.g. \"hanka\", \"nanka\", etc.).\n",
    "    This is only a demo; you'll want to expand it for\n",
    "    ho-, no-, hi-, ni-, ha-, na-, etc.\n",
    "    \"\"\"\n",
    "    variants = set([prefix_base])  # always include the original\n",
    "    \n",
    "    # If the prefix starts with 'm', then we can replace 'm' with 'h' or 'n'.\n",
    "    if prefix_base.startswith(\"m\"):\n",
    "        variants.add(\"h\" + prefix_base[1:])\n",
    "        variants.add(\"n\" + prefix_base[1:])\n",
    "    else:\n",
    "        # If it doesn't start with 'm', we might just prepend 'h' or 'n'.\n",
    "        variants.add(\"h\" + prefix_base)\n",
    "        variants.add(\"n\" + prefix_base)\n",
    "        # Optionally add 'hi'+prefix_base, 'ho'+prefix_base, etc.\n",
    "        # variants.add(\"hi\" + prefix_base)\n",
    "        # variants.add(\"ho\" + prefix_base)\n",
    "        # ...\n",
    "\n",
    "    return variants\n",
    "\n",
    "\n",
    "#####################################################\n",
    "# 4. HELPER FUNCTIONS TO REMOVE EACH TYPE OF AFFIX\n",
    "#####################################################\n",
    "\n",
    "def try_remove_prefix(word, prefix):\n",
    "    \"\"\"\n",
    "    Attempt to remove `prefix` from the start of `word`.\n",
    "    - Also tries to handle boundary mutations if `prefix` ends with 'n'.\n",
    "    - Returns (was_removed, new_word).\n",
    "    \"\"\"\n",
    "    # 1) Direct match check\n",
    "    if word.startswith(prefix):\n",
    "        return True, word[len(prefix):]\n",
    "\n",
    "    # 2) If direct match fails, try boundary mutation reversal\n",
    "    #    Example: prefix ends with 'n' => might be n+b -> \"mb\", n+p -> \"mp\", etc.\n",
    "    if prefix.endswith('n'):\n",
    "        prefix_minus_n = prefix[:-1]\n",
    "        for mutated_boundary, original_pairs in revert_boundary_map.items():\n",
    "            # We see if the word starts with prefix_minus_n + mutated_boundary\n",
    "            candidate = prefix_minus_n + mutated_boundary\n",
    "            if word.startswith(candidate):\n",
    "                # Then we guess it was \"n\"+\"b\" => \"mb\" or \"n\"+\"p\" => \"mp\", etc.\n",
    "                # So we remove the entire chunk from the start of the word\n",
    "                new_word = word[len(candidate):]\n",
    "                return True, new_word\n",
    "\n",
    "    return False, word\n",
    "\n",
    "\n",
    "def try_remove_suffix(word, suffix):\n",
    "    \"\"\"\n",
    "    Attempt to remove `suffix` from the end of `word`.\n",
    "    - Similar boundary logic can be used if suffix starts with certain letters.\n",
    "    - Returns (was_removed, new_word).\n",
    "    \"\"\"\n",
    "    if word.endswith(suffix):\n",
    "        return True, word[:-len(suffix)]\n",
    "    \n",
    "    # If needed, implement boundary logic for suffix\n",
    "    # (like \"mb\", \"mp\" at the end, etc.)\n",
    "    \n",
    "    return False, word\n",
    "\n",
    "\n",
    "def try_remove_circumfix(word, prefix_part, suffix_part):\n",
    "    \"\"\"\n",
    "    Attempt to remove a circumfix: 'prefix_part' at the start, \n",
    "    'suffix_part' at the end. Example: \"aha-ana\" => prefix=\"aha\", suffix=\"ana\".\n",
    "    - Returns (was_removed, new_word).\n",
    "    \"\"\"\n",
    "    if word.startswith(prefix_part) and word.endswith(suffix_part):\n",
    "        new_word = word[len(prefix_part):-len(suffix_part)]\n",
    "        return True, new_word\n",
    "    return False, word\n",
    "\n",
    "\n",
    "#####################################################\n",
    "# 5. THE MAIN RULE-BASED STEM FUNCTION\n",
    "#####################################################\n",
    "\n",
    "def rule_based_stem(word, affixes):\n",
    "\n",
    "    changed = True\n",
    "    while changed:\n",
    "        changed = False\n",
    "\n",
    "        # 1. Try to remove recognized prefixes (type=1).\n",
    "        #    We'll also generate tense variants for each prefix and try those.\n",
    "        for aff in affixes:\n",
    "            if aff['type'] == 1:  # prefix\n",
    "                base_prefix = aff['affix']\n",
    "                variants = generate_tense_variants(base_prefix)\n",
    "                # Try each variant\n",
    "                for varpref in variants:\n",
    "                    removed, new_word = try_remove_prefix(word, varpref)\n",
    "                    if removed:\n",
    "                        word = new_word\n",
    "                        changed = True\n",
    "                        break  # prefix removed, restart\n",
    "                if changed:\n",
    "                    break  # prefix removal succeeded, re-check from scratch\n",
    "        \n",
    "        if changed:\n",
    "            continue  # re-check from start\n",
    "\n",
    "        # 2. Try to remove recognized suffixes (type=2).\n",
    "        for aff in affixes:\n",
    "            if aff['type'] == 2:  # suffix\n",
    "                removed, new_word = try_remove_suffix(word, aff['affix'])\n",
    "                if removed:\n",
    "                    word = new_word\n",
    "                    changed = True\n",
    "                    break\n",
    "        if changed:\n",
    "            continue\n",
    "\n",
    "        # 3. Try to remove recognized circumfixes (type=3).\n",
    "        #    e.g. \"aha-ana\" => prefix_part=\"aha\", suffix_part=\"ana\"\n",
    "        for aff in affixes:\n",
    "            if aff['type'] == 3:\n",
    "                parts = aff['affix'].split('-')\n",
    "                if len(parts) == 2:\n",
    "                    prefix_part, suffix_part = parts\n",
    "                    removed, new_word = try_remove_circumfix(word, prefix_part, suffix_part)\n",
    "                    if removed:\n",
    "                        word = new_word\n",
    "                        changed = True\n",
    "                        break\n",
    "        if changed:\n",
    "            continue\n",
    "        \n",
    "        # 4. Try to remove infixes (type=4), e.g. \"-al-\"\n",
    "        for aff in affixes:\n",
    "            if aff['type'] == 4:\n",
    "                # For a minimal example, just remove the substring (without the dashes).\n",
    "                infix_str = aff['affix'].strip('-')\n",
    "                if infix_str in word:\n",
    "                    word = word.replace(infix_str, \"\", 1)  # remove first occurrence\n",
    "                    changed = True\n",
    "                    break\n",
    "        if changed:\n",
    "            continue\n",
    "        \n",
    "        # 5. Try to remove special apostrophe-based suffixes (type=5), e.g. \"-'ny\"\n",
    "        for aff in affixes:\n",
    "            if aff['type'] == 5:\n",
    "                # Example: \"-'ny\"\n",
    "                # We can simply see if word ends with \"'ny\" after ignoring the leading dash.\n",
    "                suffix_str = aff['affix'][1:]  # remove the leading '-'\n",
    "                if word.endswith(suffix_str):\n",
    "                    word = word[:-len(suffix_str)]\n",
    "                    changed = True\n",
    "                    break\n",
    "\n",
    "    return word\n",
    "\n",
    "\n",
    "#####################################################\n",
    "# 6. SIMPLE DEMO / TEST\n",
    "#####################################################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 1) Load the affixes from CSV\n",
    "    affixes = load_affixes_from_csv(\"malagasy_affixes.csv\")\n",
    "    \n",
    "    # 2) Some test words\n",
    "    test_words = [\n",
    "        \"mankabidy\",   # \"manka\" + \"bidy\" => possibly \"bidy\"\n",
    "        \"hankabidy\",   # tense variant => \"bidy\"\n",
    "        \"mambabo\",     # \"man\" + \"babo\" => mutated boundary = \"mambabo\" => root \"babo\"\n",
    "        \"abiliana\",    # from your ML code example\n",
    "        \"mialana\",     # possibly \"mi\" + \"alana\" or other combos\n",
    "        \"zaka-tsaka\",  # compound/hyphen example (not fully handled here)\n",
    "    ]\n",
    "    \n",
    "    print(\"Rule-Based Stemming Demo:\\n\")\n",
    "    for w in test_words:\n",
    "        stemmed = rule_based_stem(w, affixes)\n",
    "        print(f\"Word: {w:15s} => Stem: {stemmed}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "012c810f-d30a-454a-ac88-745d6493b60a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 44118 derivative-root pairs from 'dictionary.db'.\n",
      "Loaded 200 affixes from 'malagasy_affixes.csv'.\n",
      "Rule-Based Exact-Match F1: 0.0825\n",
      "Example: mankabidy => bidy\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sqlite3\n",
    "import csv\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "#####################################################\n",
    "# 1. LOAD DATA FROM THE DATABASE\n",
    "#####################################################\n",
    "\n",
    "def load_derivative_root_pairs(db_path=\"dictionary.db\"):\n",
    "    \"\"\"\n",
    "    Connect to the SQLite database and retrieve all derivative-root pairs.\n",
    "    Returns a tuple (inflected_words, root_words) as lists of strings.\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"\"\"\n",
    "        SELECT d.derivative, r.root\n",
    "        FROM derivatives d\n",
    "        JOIN root_words r ON d.root_id = r.id\n",
    "    \"\"\")\n",
    "    data = cursor.fetchall()\n",
    "    conn.close()\n",
    "\n",
    "    if not data:\n",
    "        print(\"No data found in the database.\")\n",
    "        return [], []\n",
    "\n",
    "    inflected_words, root_words = zip(*data)  # unzip into two lists\n",
    "    return list(inflected_words), list(root_words)\n",
    "\n",
    "\n",
    "#####################################################\n",
    "# 2. LOAD AFFIXES FROM CSV\n",
    "#####################################################\n",
    "\n",
    "def load_affixes_from_csv(filepath=\"affixes.csv\"):\n",
    "    \"\"\"\n",
    "    Load a list of affixes from a CSV with columns: \"affix\",\"type\".\n",
    "    Example rows:\n",
    "       affix,type\n",
    "       manka,1\n",
    "       ana,2\n",
    "       aha-ana,3\n",
    "       -al-,4\n",
    "       -'ny,5\n",
    "    Returns a list of dicts like:\n",
    "       [ {\"affix\": \"manka\", \"type\": 1},\n",
    "         {\"affix\": \"ana\",   \"type\": 2},\n",
    "         ... ]\n",
    "    \"\"\"\n",
    "    affixes = []\n",
    "    with open(filepath, mode='r', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            affix_str = row['affix'].strip()\n",
    "            affix_type = int(row['type'])\n",
    "            affixes.append({\n",
    "                'affix': affix_str,\n",
    "                'type': affix_type\n",
    "            })\n",
    "    return affixes\n",
    "\n",
    "\n",
    "#####################################################\n",
    "# 3. CONSONANT MUTATION & TENSE VARIANTS (SKELETON)\n",
    "#####################################################\n",
    "\n",
    "# A map for \"reverse-engineering\" mutated boundary segments.\n",
    "# Key = mutated boundary, Value = list of possible (prefix_char, root_char) combos.\n",
    "revert_boundary_map = {\n",
    "    \"mb\":  [(\"n\", \"b\")],\n",
    "    \"mp\":  [(\"n\", \"p\")],\n",
    "    \"ng\":  [(\"n\", \"h\")],\n",
    "    \"nk\":  [(\"n\", \"h\")],\n",
    "    \"nd\":  [(\"n\", \"l\")],\n",
    "    \"m\":   [(\"n\", \"p\"), (\"n\", \"b\"), (\"n\", \"f\"), (\"n\", \"v\"), (\"n\", \"m\")],\n",
    "    \"n\":   [(\"n\", \"n\")],\n",
    "    \"ndr\": [(\"n\", \"r\")],\n",
    "    \"nts\": [(\"n\", \"s\")],\n",
    "    \"nj\":  [(\"n\", \"j\")],\n",
    "}\n",
    "\n",
    "def generate_tense_variants(prefix_base):\n",
    "    \"\"\"\n",
    "    Return possible tense/polarity variants for a given prefix.\n",
    "    For example, if prefix_base=\"manka\", generate \"manka\", \"hanka\", \"nanka\", etc.\n",
    "    Expand as needed (e.g. ho, no, hi, ni, ha, na).\n",
    "    \"\"\"\n",
    "    variants = set([prefix_base])\n",
    "    \n",
    "    if prefix_base.startswith(\"m\"):\n",
    "        variants.add(\"h\" + prefix_base[1:])\n",
    "        variants.add(\"n\" + prefix_base[1:])\n",
    "    else:\n",
    "        # Could also add hi-, ho-, ha-, ni-, no-, na- if appropriate\n",
    "        variants.add(\"h\" + prefix_base)\n",
    "        variants.add(\"n\" + prefix_base)\n",
    "    \n",
    "    return variants\n",
    "\n",
    "\n",
    "#####################################################\n",
    "# 4. HELPER FUNCTIONS TO STRIP AFFIXES\n",
    "#####################################################\n",
    "\n",
    "def try_remove_prefix(word, prefix):\n",
    "    \"\"\"\n",
    "    Attempt to remove `prefix` from the start of `word`.\n",
    "    Handles boundary mutation if prefix ends with 'n'.\n",
    "    Returns: (was_removed, new_word).\n",
    "    \"\"\"\n",
    "    # Check direct match\n",
    "    if word.startswith(prefix):\n",
    "        return True, word[len(prefix):]\n",
    "\n",
    "    # If prefix ends with 'n', see if there's a mutated boundary\n",
    "    if prefix.endswith('n'):\n",
    "        prefix_minus_n = prefix[:-1]\n",
    "        for mutated_boundary, original_pairs in revert_boundary_map.items():\n",
    "            candidate = prefix_minus_n + mutated_boundary\n",
    "            if word.startswith(candidate):\n",
    "                # So we remove candidate from the start\n",
    "                return True, word[len(candidate):]\n",
    "    \n",
    "    # If nothing matches\n",
    "    return False, word\n",
    "\n",
    "\n",
    "def try_remove_suffix(word, suffix):\n",
    "    \"\"\"\n",
    "    Attempt to remove `suffix` from the end of `word`.\n",
    "    (You can expand to handle boundary mutation at the end.)\n",
    "    Returns: (was_removed, new_word).\n",
    "    \"\"\"\n",
    "    if word.endswith(suffix):\n",
    "        return True, word[:-len(suffix)]\n",
    "    return False, word\n",
    "\n",
    "\n",
    "def try_remove_circumfix(word, prefix_part, suffix_part):\n",
    "    \"\"\"\n",
    "    Attempt to remove a circumfix: prefix_part ... suffix_part\n",
    "    e.g., \"aha-ana\" => prefix=\"aha\", suffix=\"ana\".\n",
    "    Returns: (was_removed, new_word).\n",
    "    \"\"\"\n",
    "    if word.startswith(prefix_part) and word.endswith(suffix_part):\n",
    "        new_word = word[len(prefix_part):-len(suffix_part)]\n",
    "        return True, new_word\n",
    "    return False, word\n",
    "\n",
    "\n",
    "#####################################################\n",
    "# 5. MAIN RULE-BASED STEM FUNCTION\n",
    "#####################################################\n",
    "\n",
    "def rule_based_stem(word, affixes):\n",
    "    \"\"\"\n",
    "    Naive approach:\n",
    "    1) Loop until no more changes:\n",
    "       - Try removing recognized prefixes (with tense variants)\n",
    "       - Try removing recognized suffixes\n",
    "       - Try removing recognized circumfixes\n",
    "       - Try removing recognized infixes\n",
    "       - Try removing apostrophe-based suffixes\n",
    "    2) Return final 'word' as stem.\n",
    "    \"\"\"\n",
    "    changed = True\n",
    "    while changed:\n",
    "        changed = False\n",
    "\n",
    "        # 1) Prefixes\n",
    "        for aff in affixes:\n",
    "            if aff['type'] == 1:  # prefix\n",
    "                base_prefix = aff['affix']\n",
    "                variants = generate_tense_variants(base_prefix)\n",
    "                for varpref in variants:\n",
    "                    removed, new_word = try_remove_prefix(word, varpref)\n",
    "                    if removed:\n",
    "                        word = new_word\n",
    "                        changed = True\n",
    "                        break\n",
    "                if changed:\n",
    "                    break\n",
    "        if changed:\n",
    "            continue\n",
    "\n",
    "        # 2) Suffixes\n",
    "        for aff in affixes:\n",
    "            if aff['type'] == 2:  # suffix\n",
    "                removed, new_word = try_remove_suffix(word, aff['affix'])\n",
    "                if removed:\n",
    "                    word = new_word\n",
    "                    changed = True\n",
    "                    break\n",
    "        if changed:\n",
    "            continue\n",
    "\n",
    "        # 3) Circumfixes\n",
    "        for aff in affixes:\n",
    "            if aff['type'] == 3:\n",
    "                parts = aff['affix'].split('-')\n",
    "                if len(parts) == 2:\n",
    "                    prefix_part, suffix_part = parts\n",
    "                    removed, new_word = try_remove_circumfix(word, prefix_part, suffix_part)\n",
    "                    if removed:\n",
    "                        word = new_word\n",
    "                        changed = True\n",
    "                        break\n",
    "        if changed:\n",
    "            continue\n",
    "\n",
    "        # 4) Infixes\n",
    "        for aff in affixes:\n",
    "            if aff['type'] == 4:\n",
    "                # e.g. affix might be \"-al-\"\n",
    "                infix_str = aff['affix'].strip('-')\n",
    "                if infix_str in word:\n",
    "                    # remove first occurrence\n",
    "                    word = word.replace(infix_str, \"\", 1)\n",
    "                    changed = True\n",
    "                    break\n",
    "        if changed:\n",
    "            continue\n",
    "\n",
    "        # 5) Apostrophe-based suffixes (type=5)\n",
    "        for aff in affixes:\n",
    "            if aff['type'] == 5:\n",
    "                # e.g. \"-'ny\"\n",
    "                suffix_str = aff['affix'][1:]  # remove the leading '-'\n",
    "                if word.endswith(suffix_str):\n",
    "                    word = word[:-len(suffix_str)]\n",
    "                    changed = True\n",
    "                    break\n",
    "\n",
    "    return word\n",
    "\n",
    "\n",
    "#####################################################\n",
    "# 6. EVALUATION (Exact-match F1)\n",
    "#####################################################\n",
    "\n",
    "def evaluate_rule_based_exact(inflected_words, root_words, affixes):\n",
    "    \"\"\"\n",
    "    Check how often the rule-based stem equals the true root (exact match).\n",
    "    Returns float F1 (which for binary exact-match is same as precision=recall=F1).\n",
    "    \"\"\"\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for derived, gold_root in zip(inflected_words, root_words):\n",
    "        guess_root = rule_based_stem(derived, affixes)\n",
    "        # We'll say \"1\" if the guess == gold, \"0\" otherwise\n",
    "        y_pred.append(1 if guess_root == gold_root else 0)\n",
    "        y_true.append(1)  # always 1 for gold\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    return f1\n",
    "\n",
    "\n",
    "#####################################################\n",
    "# 7. MAIN PROGRAM\n",
    "#####################################################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 1) Load the derivative-root pairs from the database\n",
    "    db_path = \"dictionary.db\"\n",
    "    inflected_words, root_words = load_derivative_root_pairs(db_path)\n",
    "\n",
    "    if not inflected_words:\n",
    "        print(\"No data to process. Exiting.\")\n",
    "        exit()\n",
    "\n",
    "    print(f\"Loaded {len(inflected_words)} derivative-root pairs from '{db_path}'.\")\n",
    "\n",
    "    # 2) Load the affixes from CSV\n",
    "    affixes_csv_path = \"malagasy_affixes.csv\"\n",
    "    affixes = load_affixes_from_csv(affixes_csv_path)\n",
    "    print(f\"Loaded {len(affixes)} affixes from '{affixes_csv_path}'.\")\n",
    "\n",
    "    # 3) Evaluate the rule-based approach (Exact-match F1)\n",
    "    f1_exact = evaluate_rule_based_exact(inflected_words, root_words, affixes)\n",
    "    print(f\"Rule-Based Exact-Match F1: {f1_exact:.4f}\")\n",
    "\n",
    "    # 4) Demo on a single sample\n",
    "    sample_word = \"mankabidy\"\n",
    "    sample_stem = rule_based_stem(sample_word, affixes)\n",
    "    print(f\"Example: {sample_word} => {sample_stem}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1816770b-3fd7-4af5-becd-6d5fea0d5570",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
